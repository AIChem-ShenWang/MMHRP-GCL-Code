{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-SWV5OnRUX8B"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xNzvBiZXUix5"},"outputs":[],"source":["!pip install rdkit\n","!pip install torch_geometric\n","!pip install bertviz"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"q4jQNF8mUjzI","executionInfo":{"status":"ok","timestamp":1726557659140,"user_tz":-480,"elapsed":14026,"user":{"displayName":"Shen Wang","userId":"04706405006580685920"}}},"outputs":[],"source":["import os\n","import sys\n","import torch\n","sys.path.append(\"/content/drive/MyDrive/MMHRP\")\n","from utils.rxn import *\n","from utils.molecule import *\n","from torch_geometric.loader import DataLoader\n","from models.MMHRP import *\n","import time\n","from tqdm import tqdm\n","import datetime\n","from sklearn.metrics import mean_absolute_error as MAE\n","import warnings\n","warnings.simplefilter('ignore')\n","import torch\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from bertviz import head_view\n","from IPython.display import HTML\n","from rdkit import Chem\n","from rdkit.Chem import AllChem, Draw"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zxknyi6MUuLv"},"outputs":[],"source":["# 1.Buchwald-Hartwig HTE\n","# import data\n","data = pd.read_excel(\"/content/drive/MyDrive/MMHRP/data/BH_HTE/BH_HTE_data.xlsx\")\n","vocab_path = \"/content/drive/MyDrive/MMHRP/utils/BH_vocab.txt\"\n","\n","# build dataset & dataloader\n","rxn_RxnSmi = list()\n","max_len = -1\n","for batch in range(data.shape[0]):\n","    RxnSmi = get_Buchwald_RxnSmi(data.iloc[batch, :])\n","    max_len = max(max_len, len(RxnSmi))\n","    RxnSmi = \" \".join(smi_tokenizer(RxnSmi))\n","    rxn_RxnSmi.append(RxnSmi)\n","\n","rxn_dataset = list()\n","smi_inputsize = 128\n","\n","for batch in tqdm(range(data.shape[0])):\n","    meta = list()\n","    # rea\n","    rea = data.loc[batch][\"aryl_halide_smiles\"]\n","    pro = data.loc[batch][\"product_smiles\"]\n","    meta.append(smis_to_graph([rea, pro]))\n","    # add\n","    base = data.loc[batch][\"base_smiles\"]\n","    ligand = data.loc[batch][\"ligand_smiles\"]\n","    additive = data.loc[batch][\"additive_smiles\"]\n","    meta.append(smis_to_graph([base, ligand, additive]))\n","    # RxnSmi\n","    RxnSmi_vec = RxnSmi_to_tensor(RxnSmi=rxn_RxnSmi[batch], maxlen_=max_len, victor_size=smi_inputsize,\n","                                  file=vocab_path)\n","    meta.append(RxnSmi_vec)\n","\n","    # yield\n","    meta.append(data.loc[batch][\"yield\"] / 100)\n","\n","    rxn_dataset.append(meta)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CgFDZZ4lWcGP"},"outputs":[],"source":["# num_list = [i for i in range(0, len(rxn_dataset), 100)]\n","num_list = [2700, 1700, 100]\n","for num in tqdm(range(len(num_list))):\n","  # import model\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  model = torch.load(\"/content/drive/MyDrive/MMHRP/exp/Interpretability/Transformer_Explainer/BH_model.pth\", map_location=device)\n","  TransEncoderLayer = model.RxnSmiEncoder[0]\n","  exp_num = num_list[num]\n","\n","  # create document\n","  dir_path = \"/content/drive/MyDrive/MMHRP/exp/Interpretability/Transformer_Explainer/BH_ExpNum=%s\" % exp_num\n","  if not os.path.exists(dir_path):\n","    os.mkdir(\"%s\" % dir_path)\n","\n","  # create data\n","  exp_data = get_Buchwald_RxnSmi(data.iloc[exp_num, :])\n","  exp_vec = RxnSmi_to_tensor(RxnSmi=\" \".join(smi_tokenizer(exp_data)), maxlen_=max_len, victor_size=smi_inputsize, file=vocab_path).to(device)\n","  exp_vec = TransEncoderLayer.norm1(exp_vec).unsqueeze(0)\n","  tokens = smi_tokenizer(exp_data)\n","  sq_len = len(tokens)\n","  with open(\"%s/BH_transformer_rxn.txt\" % dir_path, 'w', encoding='utf-8') as f:\n","    for token in tokens:\n","      f.write(\"%s \" % token)\n","\n","  # Drawing of the reaction\n","  rxn = \"\".join(tokens)\n","  reactants, reagents, products = rxn.split(\">\")\n","  reactants = reactants.split(\".\")\n","  reactants = [Chem.MolFromSmiles(i) for i in reactants]\n","  reagents = reagents.split(\".\")\n","  if len(reagents) != 0:\n","    reagents = [Chem.MolFromSmiles(i) for i in reagents]\n","  products = products.split(\".\")\n","  products = [Chem.MolFromSmiles(i) for i in products]\n","  mols = reactants + reagents + products\n","  labels = []\n","  for i in range(len(reactants)):\n","    labels.append(\"Reactant %d\" % (i + 1))\n","  for i in range(len(reagents)):\n","    labels.append(\"Reagent %d\" % (i + 1))\n","  for i in range(len(products)):\n","    labels.append(\"Product %d\" % (i + 1))\n","  img = Draw.MolsToGridImage(mols, molsPerRow=len(mols), subImgSize=(300, 300), legends=labels, returnPNG=False)\n","  img.save(\"%s/Reaction.png\" % dir_path)\n","\n","  # MultiheadAttention\n","  attn_output, attn_output_weights = TransEncoderLayer.self_attn(exp_vec, exp_vec, exp_vec, average_attn_weights=False)\n","  attn = attn_output_weights[:, :, :sq_len, :sq_len]\n","  attn_norm = (attn - attn.min()) / (attn.max() - attn.min())\n","  html_content = head_view([attn_norm], tokens, html_action=\"return\").data\n","  with open(\"%s/BH_transformer_MultiHeadAttn.html\" % dir_path, 'w', encoding='utf-8') as f:\n","    f.write(html_content)\n","\n","  # Avg MultiheadAttention\n","  attn_output, avg_attn_output_weights = TransEncoderLayer.self_attn(exp_vec, exp_vec, exp_vec, average_attn_weights=True)\n","  avg_attn = avg_attn_output_weights[:, :sq_len, :sq_len].view(sq_len, sq_len).detach().cpu().numpy()\n","  avg_attn_norm = (avg_attn - avg_attn.min()) / (avg_attn.max() - avg_attn.min())\n","\n","  plt.figure(dpi=500)\n","  plt.imshow(avg_attn_norm)\n","  plt.xticks(range(sq_len), tokens, rotation=90, fontsize=1.8)\n","  plt.yticks(range(sq_len), tokens, fontsize=1.8)\n","  plt.colorbar()\n","  plt.title(\"Attention Metreics for Buchwald-Hartwig Reaction\")\n","  plt.tight_layout()\n","  plt.savefig(\"%s/BH_transformer_attn.png\" % dir_path)\n","\n","  # Implict Relationsip\n","  Relationship_set = set()\n","  for i in range(avg_attn_norm.shape[0]):\n","    for j in range(avg_attn_norm.shape[1]):\n","      if avg_attn_norm[i, j] >= 0.8:\n","        Relationship_set.add(\"%s->%s\" %(tokens[i], tokens[j]))\n","  print(Relationship_set)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1QC1ZpOeZg4X"},"outputs":[],"source":["# 2.Suzuki- HTE\n","# import data\n","data = pd.read_excel(\"/content/drive/MyDrive/MMHRP/data/Suzuki_HTE/Suzuki_HTE_data.xlsx\")\n","vocab_type = \"Suzuki\"\n","vocab_path = \"/content/drive/MyDrive/MMHRP/utils/%s_vocab.txt\" % vocab_type\n","\n","# Generate Rxnsmi\n","rxn_RxnSmi = list()\n","max_len = -1\n","for batch in range(data.shape[0]):\n","    RxnSmi = get_Suzuki_RxnSmi(data.iloc[batch, :])\n","    max_len = max(max_len, len(RxnSmi))\n","    RxnSmi = \" \".join(smi_tokenizer(RxnSmi))\n","    rxn_RxnSmi.append(RxnSmi)\n","\n","rxn_dataset = list()\n","smi_inputsize = 128\n","\n","for batch in tqdm(range(data.shape[0])):\n","    meta = list()\n","    # rea\n","    rea1 = data.loc[batch][\"Reactant_1_Name\"]\n","    rea2 = data.loc[batch][\"Reactant_2_Name\"]\n","    meta.append(smis_to_graph([rea1, rea2]))\n","    # add\n","    add = list()\n","\n","    base = data.loc[batch][\"Reagent_1_Short_Hand\"]\n","    if not pd.isnull(base):\n","        add.append(base)\n","    ligand = data.loc[batch][\"Ligand_Short_Hand\"]\n","    if not pd.isnull(ligand):\n","        add.append(ligand)\n","    sol = data.loc[batch][\"Solvent_1_Short_Hand\"]\n","    if not pd.isnull(sol):\n","        add.append(sol)\n","\n","    meta.append(smis_to_graph(add))\n","\n","    # RxnSmi\n","    RxnSmi_vec = RxnSmi_to_tensor(RxnSmi=rxn_RxnSmi[batch], maxlen_=max_len, victor_size=smi_inputsize,\n","                                  file=vocab_path)\n","    meta.append(RxnSmi_vec)\n","\n","    # yield\n","    meta.append(data.loc[batch][\"Product_Yield_PCT_Area_UV\"] / 100)\n","\n","    rxn_dataset.append(meta)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wnINoSyl4zpb"},"outputs":[],"source":["# num_list = [i for i in range(0, len(rxn_dataset), 100)]\n","num_list = [1600, 1500, 2300]\n","for num in tqdm(range(len(num_list))):\n","  # import model\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  model = torch.load(\"/content/drive/MyDrive/MMHRP/exp/Interpretability/Transformer_Explainer/Suzuki_model.pth\", map_location=device)\n","  TransEncoderLayer = model.RxnSmiEncoder[0]\n","  exp_num = num_list[num]\n","\n","  # create document\n","  dir_path = \"/content/drive/MyDrive/MMHRP/exp/Interpretability/Transformer_Explainer/Suzuki_ExpNum=%s\" % exp_num\n","  if not os.path.exists(dir_path):\n","    os.mkdir(\"%s\" % dir_path)\n","\n","  # create data\n","  exp_data = get_Suzuki_RxnSmi(data.iloc[exp_num, :])\n","  exp_vec = RxnSmi_to_tensor(RxnSmi=\" \".join(smi_tokenizer(exp_data)), maxlen_=max_len, victor_size=smi_inputsize, file=vocab_path).to(device)\n","  exp_vec = TransEncoderLayer.norm1(exp_vec).unsqueeze(0)\n","  tokens = smi_tokenizer(exp_data)\n","  sq_len = len(tokens)\n","  with open(\"%s/Suzuki_transformer_rxn.txt\" % dir_path, 'w', encoding='utf-8') as f:\n","    for token in tokens:\n","      f.write(\"%s \" % token)\n","\n","  # Drawing of the reaction\n","  rxn = \"\".join(tokens)\n","  reactants, reagents, products = rxn.split(\">\")\n","  reactants = reactants.split(\".\")\n","  reactants = [Chem.MolFromSmiles(i) for i in reactants]\n","  reagents = reagents.split(\".\")\n","  if len(reagents) != 0:\n","    reagents = [Chem.MolFromSmiles(i) for i in reagents]\n","  products = products.split(\".\")\n","  products = [Chem.MolFromSmiles(i) for i in products]\n","  mols = reactants + reagents + products\n","  labels = []\n","  for i in range(len(reactants)):\n","    labels.append(\"Reactant %d\" % (i + 1))\n","  for i in range(len(reagents)):\n","    labels.append(\"Reagent %d\" % (i + 1))\n","  for i in range(len(products)):\n","    labels.append(\"Product %d\" % (i + 1))\n","  img = Draw.MolsToGridImage(mols, molsPerRow=len(mols), subImgSize=(300, 300), legends=labels, returnPNG=False)\n","  img.save(\"%s/Reaction.png\" % dir_path)\n","\n","  # MultiheadAttention\n","  attn_output, attn_output_weights = TransEncoderLayer.self_attn(exp_vec, exp_vec, exp_vec, average_attn_weights=False)\n","  attn = attn_output_weights[:, :, :sq_len, :sq_len]\n","  attn_norm = (attn - attn.min()) / (attn.max() - attn.min())\n","  html_content = head_view([attn_norm], tokens, html_action=\"return\").data\n","  with open(\"%s/Suzuki_transformer_MultiHeadAttn.html\" % dir_path, 'w', encoding='utf-8') as f:\n","    f.write(html_content)\n","\n","  # Avg MultiheadAttention\n","  attn_output, avg_attn_output_weights = TransEncoderLayer.self_attn(exp_vec, exp_vec, exp_vec, average_attn_weights=True)\n","  avg_attn = avg_attn_output_weights[:, :sq_len, :sq_len].view(sq_len, sq_len).detach().cpu().numpy()\n","  avg_attn_norm = (avg_attn - avg_attn.min()) / (avg_attn.max() - avg_attn.min())\n","  plt.figure(dpi=500)\n","  plt.imshow(avg_attn_norm)\n","  plt.xticks(range(sq_len), tokens, rotation=90, fontsize=1.8)\n","  plt.yticks(range(sq_len), tokens, fontsize=1.8)\n","  plt.colorbar()\n","  plt.title(\"Attention Metreics for Suzuki-Miyaura Reaction\")\n","  plt.savefig(\"%s/Suzuki_transformer_attn.png\" % dir_path)\n","\n","  # Implict Relationsip\n","  Relationship_set = set()\n","  for i in range(avg_attn_norm.shape[0]):\n","    for j in range(avg_attn_norm.shape[1]):\n","      if avg_attn_norm[i, j] >= 0.8:\n","        Relationship_set.add(\"%s->%s\" %(tokens[i], tokens[j]))\n","  print(Relationship_set)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GrZNK5VA2Io9"},"outputs":[],"source":["# 3.Asymmetric Thiol HTE\n","# import data\n","data = pd.read_csv(\"/content/drive/MyDrive/MMHRP/data/AT/Asymmetric_Thiol_Addition.csv\")\n","vocab_type = \"AT\"\n","vocab_path = \"/content/drive/MyDrive/MMHRP/utils/%s_vocab.txt\" % vocab_type\n","\n","# Generate Rxnsmi\n","rxn_RxnSmi = list()\n","max_len = -1\n","for batch in range(data.shape[0]):\n","    RxnSmi = get_AT_RxnSmi(data.iloc[batch, :])\n","    max_len = max(max_len, len(RxnSmi))\n","    RxnSmi = \" \".join(smi_tokenizer(RxnSmi))\n","    rxn_RxnSmi.append(RxnSmi)\n","\n","rxn_dataset = list()\n","smi_inputsize = 128\n","\n","for batch in tqdm(range(data.shape[0])):\n","    meta = list()\n","    # rea\n","    rea1 = data.loc[batch][\"Imine\"]\n","    rea2 = data.loc[batch][\"Thiol\"]\n","    prod = data.loc[batch][\"product\"]\n","    meta.append(smis_to_graph([rea1, rea2, prod]))\n","    # add\n","    add = list()\n","\n","    cat = data.loc[batch][\"Catalyst\"]\n","    add.append(cat)\n","\n","    meta.append(smis_to_graph(add))\n","\n","    # RxnSmi\n","    RxnSmi_vec = RxnSmi_to_tensor(RxnSmi=rxn_RxnSmi[batch], maxlen_=max_len, victor_size=smi_inputsize,\n","                                  file=vocab_path)\n","    meta.append(RxnSmi_vec)\n","\n","    # yield\n","    meta.append(data.loc[batch][\"Output\"])\n","\n","    rxn_dataset.append(meta)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IDSWFaB5aP_t"},"outputs":[],"source":["# num_list = [i for i in range(0, len(rxn_dataset), 100)]\n","num_list = [300, 200, 100]\n","for num in tqdm(range(len(num_list))):\n","  # import model\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  model = torch.load(\"/content/drive/MyDrive/MMHRP/exp/Interpretability/Transformer_Explainer/AT_model.pth\", map_location=device)\n","  TransEncoderLayer = model.RxnSmiEncoder[0]\n","  exp_num = num_list[num]\n","\n","  # create document\n","  dir_path = \"/content/drive/MyDrive/MMHRP/exp/Interpretability/Transformer_Explainer/AT_ExpNum=%s\" % exp_num\n","  if not os.path.exists(dir_path):\n","    os.mkdir(\"%s\" % dir_path)\n","\n","  # create data\n","  exp_data = get_AT_RxnSmi(data.iloc[exp_num, :])\n","  exp_vec = RxnSmi_to_tensor(RxnSmi=\" \".join(smi_tokenizer(exp_data)), maxlen_=max_len, victor_size=smi_inputsize, file=vocab_path).to(device)\n","  exp_vec = TransEncoderLayer.norm1(exp_vec).unsqueeze(0)\n","  tokens = smi_tokenizer(exp_data)\n","  sq_len = len(tokens)\n","  with open(\"%s/AT_transformer_rxn.txt\" % dir_path, 'w', encoding='utf-8') as f:\n","    for token in tokens:\n","      f.write(\"%s \" % token)\n","\n","  # Drawing of the reaction\n","  rxn = \"\".join(tokens)\n","  reactants, reagents, products = rxn.split(\">\")\n","  reactants = reactants.split(\".\")\n","  reactants = [Chem.MolFromSmiles(i) for i in reactants]\n","  reagents = reagents.split(\".\")\n","  if len(reagents) != 0:\n","    reagents = [Chem.MolFromSmiles(i) for i in reagents]\n","  products = products.split(\".\")\n","  products = [Chem.MolFromSmiles(i) for i in products]\n","  mols = reactants + reagents + products\n","  labels = []\n","  for i in range(len(reactants)):\n","    labels.append(\"Reactant %d\" % (i + 1))\n","  for i in range(len(reagents)):\n","    labels.append(\"Reagent %d\" % (i + 1))\n","  for i in range(len(products)):\n","    labels.append(\"Product %d\" % (i + 1))\n","  img = Draw.MolsToGridImage(mols, molsPerRow=len(mols), subImgSize=(300, 300), legends=labels, returnPNG=False)\n","  img.save(\"%s/Reaction.png\" % dir_path)\n","\n","  # MultiheadAttention\n","  attn_output, attn_output_weights = TransEncoderLayer.self_attn(exp_vec, exp_vec, exp_vec, average_attn_weights=False)\n","  attn = attn_output_weights[:, :, :sq_len, :sq_len]\n","  attn_norm = (attn - attn.min()) / (attn.max() - attn.min())\n","  html_content = head_view([attn_norm], tokens, html_action=\"return\").data\n","  with open(\"%s/AT_transformer_MultiHeadAttn.html\" % dir_path, 'w', encoding='utf-8') as f:\n","    f.write(html_content)\n","\n","  # Avg MultiheadAttention\n","  attn_output, avg_attn_output_weights = TransEncoderLayer.self_attn(exp_vec, exp_vec, exp_vec, average_attn_weights=True)\n","  avg_attn = avg_attn_output_weights[:, :sq_len, :sq_len].view(sq_len, sq_len).detach().cpu().numpy()\n","  avg_attn_norm = (avg_attn - avg_attn.min()) / (avg_attn.max() - avg_attn.min())\n","  plt.figure(dpi=500)\n","  plt.imshow(avg_attn_norm)\n","  plt.xticks(range(sq_len), tokens, rotation=90, fontsize=1.8)\n","  plt.yticks(range(sq_len), tokens, fontsize=1.8)\n","  plt.colorbar()\n","  plt.title(\"Attention Metreics for Asymmetric Thiol Reaction\")\n","  plt.savefig(\"%s/AT_transformer_attn.png\" % dir_path)\n","\n","  # Implict Relationsip\n","  Relationship_set = set()\n","  for i in range(avg_attn_norm.shape[0]):\n","    for j in range(avg_attn_norm.shape[1]):\n","      if avg_attn_norm[i, j] >= 0.8:\n","        Relationship_set.add(\"%s->%s\" %(tokens[i], tokens[j]))\n","  print(Relationship_set)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-GA8n71jadjm"},"outputs":[],"source":["# 4. SNAR Literature\n","# import data\n","data = pd.read_excel(\"/content/drive/MyDrive/MMHRP/data/SNAR/SNAR_data.xlsx\")\n","vocab_type = \"SNAR\"\n","vocab_path = \"/content/drive/MyDrive/MMHRP/utils/%s_vocab.txt\" % vocab_type\n","\n","# Generate Rxnsmi\n","rxn_RxnSmi = list()\n","max_len = -1\n","for batch in range(data.shape[0]):\n","    RxnSmi = get_SNAR_RxnSmi(data.iloc[batch, :])\n","    max_len = max(max_len, len(RxnSmi))\n","    RxnSmi = \" \".join(smi_tokenizer(RxnSmi))\n","    rxn_RxnSmi.append(RxnSmi)\n","\n","rxn_dataset = list()\n","smi_inputsize = 128\n","\n","for batch in tqdm(range(data.shape[0])):\n","    meta = list()\n","    # rea\n","    rea1 = data.loc[batch][\"Substrate SMILES\"]\n","    rea2 = data.loc[batch][\"Nucleophile SMILES\"]\n","    prod = data.loc[batch][\"Product SMILES\"]\n","    meta.append(smis_to_graph([rea1, rea2, prod]))\n","    # sol\n","    sol = list()\n","\n","    sol = data.loc[batch][\"Solvent\"].split(\".\")\n","\n","    meta.append(smis_to_graph(sol))\n","\n","    # RxnSmi\n","    RxnSmi_vec = RxnSmi_to_tensor(RxnSmi=rxn_RxnSmi[batch], maxlen_=max_len, victor_size=smi_inputsize,\n","                                  file=vocab_path)\n","    meta.append(RxnSmi_vec)\n","\n","    # activation energy\n","    meta.append(data.loc[batch][\"exp_activation_energy\"])\n","\n","    rxn_dataset.append(meta)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v0PJD3uwanVe"},"outputs":[],"source":["# num_list = [i for i in range(0, len(rxn_dataset), 100)]\n","num_list = [200, 150, 50]\n","for num in tqdm(range(len(num_list))):\n","  # import model\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  model = torch.load(\"/content/drive/MyDrive/MMHRP/exp/Interpretability/Transformer_Explainer/SNAR_model.pth\", map_location=device)\n","  TransEncoderLayer = model.RxnSmiEncoder[0]\n","  exp_num = num_list[num]\n","\n","  # create document\n","  dir_path = \"/content/drive/MyDrive/MMHRP/exp/Interpretability/Transformer_Explainer/SNAR_ExpNum=%s\" % exp_num\n","  if not os.path.exists(dir_path):\n","    os.mkdir(\"%s\" % dir_path)\n","\n","  # create data\n","  exp_data = get_SNAR_RxnSmi(data.iloc[exp_num, :])\n","  exp_vec = RxnSmi_to_tensor(RxnSmi=\" \".join(smi_tokenizer(exp_data)), maxlen_=max_len, victor_size=smi_inputsize, file=vocab_path).to(device)\n","  exp_vec = TransEncoderLayer.norm1(exp_vec).unsqueeze(0)\n","  tokens = smi_tokenizer(exp_data)\n","  sq_len = len(tokens)\n","  with open(\"%s/SNAR_transformer_rxn.txt\" % dir_path, 'w', encoding='utf-8') as f:\n","    for token in tokens:\n","      f.write(\"%s \" % token)\n","\n","  # Drawing of the reaction\n","  rxn = \"\".join(tokens)\n","  reactants, reagents, products = rxn.split(\">\")\n","  reactants = reactants.split(\".\")\n","  reactants = [Chem.MolFromSmiles(i) for i in reactants]\n","  reagents = reagents.split(\".\")\n","  if len(reagents) != 0:\n","    reagents = [Chem.MolFromSmiles(i) for i in reagents]\n","  products = products.split(\".\")\n","  products = [Chem.MolFromSmiles(i) for i in products]\n","  mols = reactants + reagents + products\n","  labels = []\n","  for i in range(len(reactants)):\n","    labels.append(\"Reactant %d\" % (i + 1))\n","  for i in range(len(reagents)):\n","    labels.append(\"Reagent %d\" % (i + 1))\n","  for i in range(len(products)):\n","    labels.append(\"Product %d\" % (i + 1))\n","  img = Draw.MolsToGridImage(mols, molsPerRow=len(mols), subImgSize=(300, 300), legends=labels, returnPNG=False)\n","  img.save(\"%s/Reaction.png\" % dir_path)\n","\n","  # MultiheadAttention\n","  attn_output, attn_output_weights = TransEncoderLayer.self_attn(exp_vec, exp_vec, exp_vec, average_attn_weights=False)\n","  attn = attn_output_weights[:, :, :sq_len, :sq_len]\n","  attn_norm = (attn - attn.min()) / (attn.max() - attn.min())\n","  html_content = head_view([attn_norm], tokens, html_action=\"return\").data\n","  with open(\"%s/SNAR_transformer_MultiHeadAttn.html\" % dir_path, 'w', encoding='utf-8') as f:\n","    f.write(html_content)\n","\n","  # Avg MultiheadAttention\n","  attn_output, avg_attn_output_weights = TransEncoderLayer.self_attn(exp_vec, exp_vec, exp_vec, average_attn_weights=True)\n","  avg_attn = avg_attn_output_weights[:, :sq_len, :sq_len].view(sq_len, sq_len).detach().cpu().numpy()\n","  avg_attn_norm = (avg_attn - avg_attn.min()) / (avg_attn.max() - avg_attn.min())\n","  plt.figure(dpi=500)\n","  plt.imshow(avg_attn_norm)\n","  plt.xticks(range(sq_len), tokens, rotation=90, fontsize=1.8)\n","  plt.yticks(range(sq_len), tokens, fontsize=1.8)\n","  plt.colorbar()\n","  plt.title(\"Attention Metreics for S$_N$Ar Reaction\")\n","  plt.savefig(\"%s/SNAR_transformer_attn.png\" % dir_path)\n","\n","  # Implict Relationsip\n","  Relationship_set = set()\n","  for i in range(avg_attn_norm.shape[0]):\n","    for j in range(avg_attn_norm.shape[1]):\n","      if avg_attn_norm[i, j] >= 0.8:\n","        Relationship_set.add(\"%s->%s\" %(tokens[i], tokens[j]))\n","  print(Relationship_set)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyPPW9PUbRYisX3alE8oQZQQ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}